"""Carga el modelo que utilizar√° el chatbot Miramar."""

from __future__ import annotations

import os

import torch
from unsloth import FastLanguageModel

from .configuracion import (
    BASE_MODEL_DIR,
    DEFAULT_MODEL_VARIANT,
    LORA_ADAPTER_DIR,
)


def _get_optimal_vram_config():
    """
    Detecta autom√°ticamente la VRAM disponible y retorna configuraci√≥n √≥ptima BALANCEADA.
    Prioriza velocidad de respuesta sobre capacidad m√°xima.
    """
    if not torch.cuda.is_available():
        return {"max_seq_length": 512, "gpu_memory_utilization": 0.5, "max_num_seqs": 8}
    
    # Detectar VRAM total
    vram_gb = torch.cuda.get_device_properties(0).total_memory / (1024**3)
    
    # CONFIGURACI√ìN BALANCEADA - Velocidad > Capacidad m√°xima
    if vram_gb >= 20:  # RTX 4090, A6000, etc.
        return {
            "max_seq_length": 2048,  # SUFICIENTE para prompts largos
            "gpu_memory_utilization": 0.8,  # REDUCIDO para estabilidad
            "max_num_seqs": 32,  # REDUCIDO para latencia baja
            "max_num_batched_tokens": 1024
        }
    elif vram_gb >= 15:  # RTX 4060 Ti 16GB, RTX 4080
        return {
            "max_seq_length": 2048,  # AUMENTADO para manejar prompts largos
            "gpu_memory_utilization": 0.75,  # MODERADO para balance
            "max_num_seqs": 16,  # REDUCIDO para menor latencia individual (de 24 a 16)
            "max_num_batched_tokens": 896  # Optimizado para velocidad (de 1024 a 896)
        }
    elif vram_gb >= 10:  # RTX 3060 Ti 12GB, RTX 4070
        return {
            "max_seq_length": 1536,
            "gpu_memory_utilization": 0.7,
            "max_num_seqs": 16,
            "max_num_batched_tokens": 768
        }
    else:  # RTX 3060 6GB, RTX 4060
        return {
            "max_seq_length": 1024,
            "gpu_memory_utilization": 0.6,
            "max_num_seqs": 8,
            "max_num_batched_tokens": 512
        }


def _load_lora_variant(device: str, disable_compile: bool = False):
    use_4bit = device == "cuda"
    
    # DETECCI√ìN AUTOM√ÅTICA DE VRAM Y CONFIGURACI√ìN √ìPTIMA
    vram_gb = 0  # Inicializar variable
    if device == "cuda":
        vram_gb = torch.cuda.get_device_properties(0).total_memory / (1024**3)
        vram_config = _get_optimal_vram_config()
        print(f"üéÆ GPU detectada: {torch.cuda.get_device_name(0)}")
        print(f"üíæ VRAM total: {vram_gb:.1f}GB")
        print(f"üöÄ Configuraci√≥n autom√°tica: {vram_config['max_seq_length']} tokens, {vram_config['gpu_memory_utilization']*100:.0f}% VRAM")
        
        if disable_compile:
            print("‚ö° MODO DESARROLLO R√ÅPIDO: torch.compile deshabilitado")
        
        torch.cuda.empty_cache()
        torch.cuda.synchronize()
        
        try:
            # Flash Attention + precision hints para mejor rendimiento
            torch.set_float32_matmul_precision("medium")
        except Exception:
            pass
    else:
        vram_config = {"max_seq_length": 512, "gpu_memory_utilization": 0.5, "max_num_seqs": 16}
    print("Cargando modelo base 4b + LoRA con optimizaciones...")
    
    # Configuraci√≥n OPTIMIZADA para RTX 4060 Ti 16GB (memoria abundante)
    # üîß CONFIGURACIONES POR VRAM DISPONIBLE:
    # 
    # üì± Para 6GB VRAM (RTX 3060, RTX 4060):
    #    "max_seq_length": 1024, "gpu_memory_utilization": 0.75, "max_num_seqs": 32
    # 
    # üéÆ Para 12GB VRAM (RTX 3060 Ti, RTX 4070):
    #    "max_seq_length": 1536, "gpu_memory_utilization": 0.8, "max_num_seqs": 48
    # 
    # üöÄ Para 16GB VRAM (RTX 4060 Ti, RTX 4080):
    #    "max_seq_length": 2048, "gpu_memory_utilization": 0.95, "max_num_seqs": 128
    #
    # üíé Para 24GB+ VRAM (RTX 4090, RTX A6000):
    #    "max_seq_length": 4096, "gpu_memory_utilization": 0.95, "max_num_seqs": 256
    
    load_kwargs = {
        "model_name": BASE_MODEL_DIR,
        "adapter_name": os.path.join(LORA_ADAPTER_DIR, "lora_adapter"),
        
        # üß† MEMORIA CONVERSACIONAL - Configuraci√≥n BALANCEADA para velocidad
        "max_seq_length": vram_config["max_seq_length"],
        
        # ‚ö° PRECISI√ìN Y VELOCIDAD - Configuraci√≥n SIMPLE
        "dtype": torch.float16 if use_4bit else torch.float32,
        "load_in_4bit": use_4bit,  # Quantizaci√≥n 4-bit esencial
        
        #  OPTIMIZACIONES B√ÅSICAS - Solo las esenciales para Unsloth
        "use_cache": True,  # Cache incremental esencial
    }
    
    model, tokenizer = FastLanguageModel.from_pretrained(**load_kwargs)
    FastLanguageModel.for_inference(model)
    
    # torch.compile HABILITADO por defecto para m√°xima velocidad (compilaci√≥n √∫nica)
    compile_enabled = False
    if device == "cuda" and hasattr(torch, "compile") and not disable_compile:
        try:
            print("üî• Compilando modelo con torch.compile...")
            print("   ‚è±Ô∏è  Esto tomar√° ~30-60s la primera vez, pero despu√©s ser√° ultra-r√°pido")
            
            # CONFIGURACI√ìN OPTIMIZADA para compilaci√≥n √∫nica:
            # - mode="default": Balance velocidad/compilaci√≥n (no "max-autotune")
            # - fullgraph=False: Evita recompilaci√≥n en grafos din√°micos
            # - dynamic=True: Maneja diferentes tama√±os de entrada sin recompilar
            model = torch.compile(
                model, 
                mode="default",      # Compilaci√≥n r√°pida, no agresiva
                fullgraph=False,     # CR√çTICO: evita recompilaci√≥n
                dynamic=True         # CR√çTICO: maneja tama√±os variables
            )
            compile_enabled = True
            print("‚úÖ Modelo compilado exitosamente - Respuestas ultra-r√°pidas habilitadas!")
        except Exception as compile_exc:
            print(f"‚ö†Ô∏è  No se pudo compilar el modelo: {compile_exc}")
            print("   Continuando sin torch.compile...")
    else:
        if disable_compile:
            print("‚úÖ Modelo SIN torch.compile (modo desarrollo r√°pido).")
        else:
            print("‚úÖ Modelo sin torch.compile (CUDA no disponible).")
    
    # Configurar tokenizer para WhatsApp (optimizaci√≥n de prompts)
    if tokenizer.pad_token_id is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    # Configurar para respuestas cortas de WhatsApp
    tokenizer.model_max_length = 2048
    
    print("‚úÖ Optimizaciones aplicadas:")
    print(f"   ‚Ä¢ Quantizaci√≥n 4-bit + KV cache FP8")
    print(f"   ‚Ä¢ Flash Attention + precision medium")
    print(f"   ‚Ä¢ Memoria conversacional: {vram_config['max_seq_length']} tokens")
    if device == "cuda":
        print(f"   ‚Ä¢ VRAM utilizada: {vram_config['gpu_memory_utilization']*100:.0f}% ({vram_config['gpu_memory_utilization']*vram_gb:.1f}GB de {vram_gb:.1f}GB)")
    print(f"   ‚Ä¢ Concurrencia m√°xima: {vram_config['max_num_seqs']} secuencias")
    if compile_enabled:
        print("   ‚Ä¢ üöÄ torch.compile activado - Respuestas ultra-r√°pidas (~2-7s)")
    elif disable_compile:
        print("   ‚Ä¢ MODO DESARROLLO: sin torch.compile (respuestas ~12-22s)")
    else:
        print("   ‚Ä¢ Sin torch.compile (CUDA no disponible)")
    
    return model, tokenizer, device


def load_model_and_tokenizer(variant: str = DEFAULT_MODEL_VARIANT):
    """
    Cargar modelo y tokenizer. Solo LoRA 4bit est√° disponible.
    
    Args:
        variant: Variante del modelo. Si contiene "fast", deshabilita torch.compile
        
    Returns:
        tuple: (model, tokenizer, device)
    """
    device = "cuda" if torch.cuda.is_available() else "cpu"
    
    # Detectar modo r√°pido basado en la variante
    disable_compile = "fast" in variant.lower()
    
    if disable_compile:
        print("‚úÖ Cargando modelo LoRA 4bit (modo desarrollo r√°pido - sin torch.compile)...")
    else:
        print("‚úÖ Cargando modelo LoRA 4bit (con torch.compile para m√°xima velocidad)...")
    
    return _load_lora_variant(device, disable_compile=disable_compile)
