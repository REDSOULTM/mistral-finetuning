#!/usr/bin/env python3
from unsloth import FastLanguageModel
"""
Fine-tuning de Mistral-7B-Instruct-v0.3 usando Unsloth
Basado en el notebook Pro copy.ipynb - VersiÃ³n final optimizada
"""

import os
import torch
import warnings
import json
warnings.filterwarnings("ignore")

from transformers import TrainingArguments
from trl import SFTTrainer
from datasets import Dataset, concatenate_datasets

def setup_model():
    """Configurar el modelo Mistral con Unsloth"""
    print("ğŸš€ Configurando modelo Mistral-7B-Instruct-v0.3...")
    
    # ConfiguraciÃ³n del modelo
    max_seq_length = 2048
    dtype = None  # Autodetect
    load_in_4bit = True
    
    # Cargar modelo y tokenizer
    model, tokenizer = FastLanguageModel.from_pretrained(
        model_name="unsloth/mistral-7b-instruct-v0.3-bnb-4bit",
        max_seq_length=max_seq_length,
        dtype=dtype,
        load_in_4bit=load_in_4bit,
    )
    
    print("âœ“ Modelo base cargado")
    
    # Configurar PEFT (LoRA)
    model = FastLanguageModel.get_peft_model(
        model,
        r=16,  # Rango LoRA
        target_modules=["q_proj", "k_proj", "v_proj", "o_proj", 
                       "gate_proj", "up_proj", "down_proj"],
        lora_alpha=16,
        lora_dropout=0,
        bias="none",
        use_gradient_checkpointing="unsloth",
        random_state=3407,
        use_rslora=False,
        loftq_config=None,
    )
    
    print("âœ“ PEFT/LoRA configurado")
    return model, tokenizer

def setup_chat_template(tokenizer):
    """Configurar template de chat para Mistral"""
    print("ğŸ“ Configurando template de chat...")
    
    # Template de chat para Mistral
    chat_template = """Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.

### Instruction:
{instruction}

### Input:
{input}

### Response:
{output}"""
    
    def formatting_prompts_func(examples):
        instructions = examples["instruction"]
        inputs = examples["input"]
        outputs = examples["output"]
        texts = []
        
        for instruction, input_text, output in zip(instructions, inputs, outputs):
            text = chat_template.format(
                instruction=instruction,
                input=input_text,
                output=output
            )
            texts.append(text)
        return {"text": texts}
    
    print("âœ“ Template de chat configurado")
    return formatting_prompts_func

def create_sample_dataset():
    """Crear dataset de ejemplo para fine-tuning"""
    print("ğŸ“Š Creando dataset de ejemplo...")
    
    # Dataset de ejemplo en espaÃ±ol
    sample_data = [
        {
            "instruction": "Responde a la siguiente pregunta de manera clara y concisa.",
            "input": "Â¿CuÃ¡l es la capital de Francia?",
            "output": "La capital de Francia es ParÃ­s. Es la ciudad mÃ¡s poblada del paÃ­s y el centro polÃ­tico, econÃ³mico y cultural de Francia."
        },
        {
            "instruction": "Explica el concepto solicitado de forma educativa.",
            "input": "Â¿QuÃ© es la inteligencia artificial?",
            "output": "La inteligencia artificial (IA) es una rama de la ciencia de la computaciÃ³n que busca crear sistemas capaces de realizar tareas que normalmente requieren inteligencia humana, como el aprendizaje, la percepciÃ³n y la toma de decisiones."
        },
        {
            "instruction": "Proporciona una respuesta Ãºtil y prÃ¡ctica.",
            "input": "Â¿CÃ³mo puedo mejorar mi productividad en el trabajo?",
            "output": "Para mejorar tu productividad: 1) Organiza tus tareas por prioridad, 2) Elimina distracciones, 3) Toma descansos regulares, 4) Usa tÃ©cnicas como Pomodoro, 5) Automatiza tareas repetitivas cuando sea posible."
        },
        {
            "instruction": "Resuelve el problema matemÃ¡tico paso a paso.",
            "input": "Si tengo 15 manzanas y regalo 7, Â¿cuÃ¡ntas me quedan?",
            "output": "Para resolver este problema: 15 manzanas iniciales - 7 manzanas regaladas = 8 manzanas restantes. Por lo tanto, te quedan 8 manzanas."
        },
        {
            "instruction": "Describe el proceso solicitado de manera clara.",
            "input": "Â¿CÃ³mo se hace cafÃ© en una cafetera francesa?",
            "output": "Para hacer cafÃ© en cafetera francesa: 1) Hierve agua, 2) Muele cafÃ© grueso, 3) AÃ±ade cafÃ© a la cafetera, 4) Vierte agua caliente, 5) Revuelve suavemente, 6) Deja reposar 4 minutos, 7) Presiona el Ã©mbolo lentamente y sirve."
        }
    ]
    
    # Crear dataset
    dataset = Dataset.from_list(sample_data)
    print(f"âœ“ Dataset creado con {len(sample_data)} ejemplos")
    return dataset

def load_miramar_dataset():
    """Cargar el dataset de Transportes Miramar desde JSONL"""
    print("ğŸš› Cargando dataset Transportes Miramar...")
    
    # Buscar el dataset en la carpeta correspondiente
    dataset_file = "../Dataset_de_Miramar/transportes_miramar_dataset_20k_20250909_044327.jsonl"
    
    if not os.path.exists(dataset_file):
        print(f"âŒ Archivo no encontrado: {dataset_file}")
        print("ğŸ’¡ Usando solo ejemplos de demostraciÃ³n")
        return None
    
    try:
        # Leer archivo JSONL
        data = []
        with open(dataset_file, 'r', encoding='utf-8') as f:
            for line_num, line in enumerate(f, 1):
                try:
                    item = json.loads(line.strip())
                    
                    # Filtrar solo ejemplos "buenos" para entrenamiento
                    if item.get('label') == 'bueno':
                        # Convertir al formato requerido
                        messages = item.get('messages', [])
                        if len(messages) >= 2:
                            user_msg = next((m['content'] for m in messages if m['role'] == 'user'), '')
                            assistant_msg = next((m['content'] for m in messages if m['role'] == 'assistant'), '')
                            
                            if user_msg and assistant_msg:
                                formatted_item = {
                                    "instruction": "Responde como experto en servicios de Transportes Miramar, proporcionando informaciÃ³n Ãºtil y profesional sobre transporte de pasajeros.",
                                    "input": user_msg,
                                    "output": assistant_msg
                                }
                                data.append(formatted_item)
                                
                except json.JSONDecodeError:
                    print(f"âš ï¸  Error en lÃ­nea {line_num}: JSON invÃ¡lido")
                    continue
                except Exception as e:
                    print(f"âš ï¸  Error procesando lÃ­nea {line_num}: {e}")
                    continue
        
        if not data:
            print("âŒ No se encontraron ejemplos vÃ¡lidos en el dataset")
            return None
        
        # Crear Dataset de Hugging Face
        dataset = Dataset.from_list(data)
        print(f"âœ… Dataset Miramar cargado: {len(dataset)} ejemplos de conversaciones exitosas")
        
        # Mostrar muestra
        print(f"ğŸ“ Ejemplo del dataset:")
        sample = data[0]
        print(f"   Usuario: {sample['input'][:80]}...")
        print(f"   Respuesta: {sample['output'][:80]}...")
        
        return dataset
        
    except Exception as e:
        print(f"âŒ Error cargando dataset Miramar: {e}")
        return None

def choose_dataset_configuration():
    """Permite elegir cÃ³mo configurar el dataset"""
    print("\nğŸ¯ CONFIGURACIÃ“N DE DATASET:")
    print("1. ğŸš› Solo Transportes Miramar (especializaciÃ³n mÃ¡xima)")
    print("2. ğŸŒ Miramar + FineTome-100k (conocimiento general + especializaciÃ³n)")
    print("3. ğŸ“š Solo FineTome-100k (dataset original)")
    
    while True:
        choice = input("Selecciona una opciÃ³n (1/2/3): ").strip()
        if choice in ['1', '2', '3']:
            return choice
        print("âŒ OpciÃ³n invÃ¡lida. Elige 1, 2 o 3")

def create_combined_dataset():
    """Combina dataset de Miramar con dataset base"""
    print("ğŸŒ Creando dataset combinado...")
    
    # Cargar dataset de Miramar
    miramar_dataset = load_miramar_dataset()
    
    # Dataset de ejemplo para demostraciÃ³n si no hay Miramar
    sample_data = [
        {
            "instruction": "Responde como experto en servicios de Transportes Miramar.",
            "input": "Â¿QuÃ© servicios ofrece Transportes Miramar?",
            "output": "Transportes Miramar ofrece servicios completos de transporte de pasajeros:\n\nğŸš› **Servicios principales:**\nâ€¢ Transporte urbano e interurbano\nâ€¢ Servicios corporativos y empresariales\nâ€¢ Traslados al aeropuerto\nâ€¢ Viajes turÃ­sticos y recreativos\nâ€¢ Transporte para eventos especiales\n\nğŸ“ Para cotizaciones y reservas, contÃ¡ctanos y te ayudaremos con una propuesta personalizada segÃºn tus necesidades."
        },
        {
            "instruction": "Proporciona informaciÃ³n Ãºtil sobre Transportes Miramar.",
            "input": "Â¿CÃ³mo puedo hacer una reserva?",
            "output": "Para hacer una reserva con Transportes Miramar:\n\nğŸ“‹ **InformaciÃ³n necesaria:**\nâ€¢ Punto de origen y destino\nâ€¢ Fecha y hora del viaje\nâ€¢ Cantidad de pasajeros\nâ€¢ Servicios adicionales requeridos\n\nğŸ“ **Canales de contacto:**\nâ€¢ TelÃ©fono directo\nâ€¢ WhatsApp empresarial\nâ€¢ Formulario en lÃ­nea\nâ€¢ Oficinas fÃ­sicas\n\nTe confirmaremos disponibilidad y te enviaremos la cotizaciÃ³n detallada de inmediato."
        }
    ]
    
    # Si no hay dataset de Miramar, usar ejemplos de demostraciÃ³n
    if miramar_dataset is None:
        print("ğŸ“ Usando ejemplos de demostraciÃ³n para Transportes Miramar")
        miramar_dataset = Dataset.from_list(sample_data)
    
    print(f"ğŸ“Š Dataset Miramar final: {len(miramar_dataset)} ejemplos")
    return miramar_dataset

def run_training(model, tokenizer, dataset, formatting_func):
    """Ejecutar el proceso de fine-tuning"""
    print("ğŸ‹ï¸ Iniciando fine-tuning...")
    print("DEBUG: tokenizer =", tokenizer)
    # Formatear dataset
    dataset = dataset.map(formatting_func, batched=True)
    
    # Configurar argumentos de entrenamiento
    training_args = TrainingArguments(
        per_device_train_batch_size=2,
        gradient_accumulation_steps=4,
        warmup_steps=5,
        max_steps=60,  # CambiÃ¡ este valor segÃºn tu entrenamiento
        learning_rate=2e-4,
        fp16=not torch.cuda.is_bf16_supported(),
        bf16=torch.cuda.is_bf16_supported(),
        logging_steps=1,
        optim="adamw_8bit",
        weight_decay=0.01,
        lr_scheduler_type="linear",
        seed=3407,
        output_dir="./outputs",
        save_steps=30,
        save_total_limit=2,
        dataloader_num_workers=0,  # Evita problemas con multiprocessing
    )
    
    print("âœ“ Argumentos de entrenamiento configurados")
    
    # Crear trainer
    # Workaround para Unsloth: adjuntar tokenizer al modelo (ambos atributos)
    model.tokenizer = tokenizer
    model._tokenizer = tokenizer
    trainer = SFTTrainer(
        model=model,
        train_dataset=dataset,
        args=training_args,
        formatting_func=formatting_func,
        tokenizer=tokenizer,  # <-- No permitido por tu versiÃ³n
    )
    print("âœ“ Trainer configurado")

    # Mostrar estadÃ­sticas antes del entrenamiento
    gpu_stats = torch.cuda.get_device_properties(0)
    start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)
    max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)
    print(f"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.")
    print(f"Memoria GPU inicial: {start_gpu_memory} GB.")

    # Entrenar
    print("\nğŸš€ Iniciando entrenamiento...")
    if trainer is not None:
        trainer_stats = trainer.train()
        
        # EstadÃ­sticas finales
        used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)
        used_memory_training = round(used_memory - start_gpu_memory, 3)
        used_percentage = round(used_memory / max_memory * 100, 3)
        print(f"\nâœ… Entrenamiento completado!")
        print(f"Memoria GPU usada: {used_memory} GB ({used_percentage}%)")
        print(f"Memoria adicional para entrenamiento: {used_memory_training} GB")
        print(f"Tiempo de entrenamiento: {trainer_stats.metrics['train_runtime']:.2f} segundos")
    else:
        print("âš ï¸  El entrenamiento fue omitido porque no se pudo crear el trainer.")
    
    return model, tokenizer, training_args.max_steps  # <-- devolvÃ© los steps

def save_model(model, tokenizer, save_directory="./mistral_finetuned", suffix=""):
    """Guardar el modelo fine-tuneado"""
    # AÃ±adir sufijo al directorio si se proporciona
    if suffix:
        save_directory = save_directory.rstrip("/") + suffix
    
    print(f"ğŸ’¾ Guardando modelo en {save_directory}...")
    
    # Crear directorio si no existe
    os.makedirs(save_directory, exist_ok=True)
    
    # Guardar modelo y tokenizer
    model.save_pretrained(save_directory)
    tokenizer.save_pretrained(save_directory)
    
    # TambiÃ©n guardar en formato GGUF (opcional)
    try:
        model.save_pretrained_gguf(save_directory, tokenizer, quantization_method="q4_k_m")
        print("âœ“ Modelo guardado en formato GGUF")
    except Exception as e:
        print(f"âš  No se pudo guardar en formato GGUF: {e}")
    
    print(f"âœ… Modelo guardado exitosamente en {save_directory}")
    return save_directory

def test_inference(model, tokenizer):
    """Probar inferencia con el modelo fine-tuneado"""
    print("\nğŸ§ª Probando inferencia con modelo fine-tuneado...")
    
    # Habilitar modo de inferencia rÃ¡pida
    FastLanguageModel.for_inference(model)
    
    # Preparar prompt de prueba
    test_instruction = "Responde a la siguiente pregunta de manera clara y concisa."
    test_input = "Â¿QuÃ© beneficios tiene hacer ejercicio regularmente?"
    
    prompt = f"""Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.

### Instruction:
{test_instruction}

### Input:
{test_input}

### Response:
"""
    
    # Tokenizar y generar
    inputs = tokenizer(prompt, return_tensors="pt").to("cuda")
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=128,
            use_cache=True,
            do_sample=True,
            temperature=0.7,
            top_p=0.9,
            pad_token_id=tokenizer.eos_token_id
        )
    
    # Decodificar respuesta
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    
    print("ğŸ“ Prompt:")
    print(prompt)
    print("\nğŸ¤– Respuesta del modelo:")
    print(response[len(prompt):].strip())
    
    return response

def test_miramar_knowledge(model, tokenizer):
    """Probar conocimiento especÃ­fico de Transportes Miramar"""
    print("\nğŸš› Probando conocimiento especÃ­fico de Transportes Miramar...")
    
    # Habilitar modo de inferencia rÃ¡pida
    FastLanguageModel.for_inference(model)
    
    # Preguntas de prueba especÃ­ficas de Transportes Miramar
    test_questions = [
        "Â¿CuÃ¡les son los horarios de Transportes Miramar?",
        "Â¿QuÃ© rutas cubre Transportes Miramar?",
        "Â¿CÃ³mo puedo contactar a Transportes Miramar?",
        "Â¿CuÃ¡l es el precio del pasaje en Transportes Miramar?",
        "Â¿Transportes Miramar tiene servicio los fines de semana?"
    ]
    
    for i, question in enumerate(test_questions, 1):
        print(f"\nğŸ“‹ Pregunta {i}: {question}")
        
        # Preparar prompt especÃ­fico para Miramar
        prompt = f"""Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.

### Instruction:
Responde como experto en servicios de Transportes Miramar, proporcionando informaciÃ³n Ãºtil y profesional sobre transporte de pasajeros.

### Input:
{question}

### Response:
"""
        
        # Tokenizar y generar
        inputs = tokenizer(prompt, return_tensors="pt").to("cuda")
        
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=100,
                use_cache=True,
                do_sample=True,
                temperature=0.7,
                top_p=0.9,
                pad_token_id=tokenizer.eos_token_id
            )
        
        # Decodificar respuesta
        response = tokenizer.decode(outputs[0], skip_special_tokens=True)
        answer = response[len(prompt):].strip()
        
        print(f"ğŸ¤– Respuesta: {answer}")
        
        # PequeÃ±a pausa entre preguntas
        import time
        time.sleep(1)
    
    print("\nâœ… Prueba de conocimiento Miramar completada")

def main():
    """FunciÃ³n principal"""
    print("ğŸ”¥ FINE-TUNING MISTRAL-7B CON UNSLOTH ğŸ”¥\n")
    
    try:
        # 1. Configurar modelo
        model, tokenizer = setup_model()
        
        # 2. Configurar template de chat
        formatting_func = setup_chat_template(tokenizer)
        
        # 3. Configurar dataset segÃºn elecciÃ³n del usuario
        choice = choose_dataset_configuration()
        
        if choice == '1':
            # Solo Transportes Miramar
            print("\nğŸš› Configurando entrenamiento con dataset Transportes Miramar...")
            dataset = load_miramar_dataset()
            if dataset is None:
                print("âŒ No se pudo cargar el dataset Miramar. Usando ejemplos de demostraciÃ³n.")
                dataset = create_sample_dataset()
            save_suffix = "_miramar_only"
            
        elif choice == '2':
            # Miramar + FineTome-100k (combinado)
            print("\nğŸŒ Configurando entrenamiento combinado (Miramar + FineTome-100k)...")
            miramar_dataset = load_miramar_dataset()
            base_dataset = create_sample_dataset()
            
            if miramar_dataset is not None:
                # Combinar datasets
                dataset = concatenate_datasets([miramar_dataset, base_dataset])
                print(f"âœ… Dataset combinado creado: {len(dataset)} ejemplos totales")
                print(f"   - Miramar: {len(miramar_dataset)} ejemplos")
                print(f"   - FineTome: {len(base_dataset)} ejemplos")
            else:
                print("âš ï¸  No se pudo cargar Miramar. Usando solo FineTome-100k")
                dataset = base_dataset
            save_suffix = "_miramar_combined"
            
        else:  # choice == '3'
            # Solo FineTome-100k
            print("\nğŸ“š Configurando entrenamiento con FineTome-100k (dataset original)...")
            dataset = create_sample_dataset()
            save_suffix = "_finetome_only"
        
        print(f"\nğŸ“Š Dataset final: {len(dataset)} ejemplos para entrenamiento\n")
        
        # 4. Ejecutar fine-tuning
        model, tokenizer, steps = run_training(model, tokenizer, dataset, formatting_func)
        
        # 5. Guardar modelo con sufijo descriptivo
        save_suffix = f"{save_suffix}_steps{steps}"  # <-- agregÃ¡ los steps al sufijo
        save_directory = save_model(model, tokenizer, suffix=save_suffix)
        
        # 6. Probar inferencia
        print("\nğŸ§ª Probando el modelo entrenado...")
        test_inference(model, tokenizer)
        
        # 7. Si usamos Miramar, hacer preguntas especÃ­ficas de prueba
        if choice in ['1', '2'] and 'miramar' in save_suffix:
            print("\nğŸš› Probando conocimiento especÃ­fico de Transportes Miramar...")
            test_miramar_knowledge(model, tokenizer)
        
        print(f"\nğŸ‰ Â¡PROCESO COMPLETADO EXITOSAMENTE! ğŸ‰")
        print(f"ğŸ“ Modelo guardado en: {save_directory}")
        print("ğŸš€ Â¡Listo para usar!")
        
    except Exception as e:
        print(f"\nâŒ Error durante el proceso: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()